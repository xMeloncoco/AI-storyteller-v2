# Dreamwalkers Backend Configuration
# Copy this to .env and update if needed

# ===== RECOMMENDED: LOCAL AI WITH OLLAMA =====
# No API keys needed, no rate limits, works offline!
# 1. Install Ollama from https://ollama.com/download
# 2. Run: ollama pull llama3.2:3b
# 3. Run: ollama pull llama3.2
# 4. That's it! Just start the backend.

AI_PROVIDER=local
SMALL_MODEL=llama3.2:3b
LARGE_MODEL=llama3.2

# ===== ALTERNATIVE: ONLINE APIs =====
# If you prefer online APIs (requires API key, has rate limits):

# AI_PROVIDER=openrouter
# OPENROUTER_API_KEY=your_key_here
# SMALL_MODEL=microsoft/phi-3-mini-128k-instruct:free
# LARGE_MODEL=google/gemma-2-9b-it:free

# OR use Nebius:
# AI_PROVIDER=nebius
# NEBIUS_API_KEY=your_key_here

# ===== OTHER SETTINGS =====

# Database
DATABASE_URL=sqlite:///./data/dreamwalkers.db

# Context Settings
MAX_CONTEXT_MESSAGES=20
MAX_TOKENS_SMALL=500
MAX_TOKENS_LARGE=2000

# Logging
LOG_LEVEL=INFO
